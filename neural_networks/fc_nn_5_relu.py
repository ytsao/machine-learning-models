import tensorflow_datasets as tfds
import tensorflow as tf

from flax import linen as nn  # Linen API
import jax
import jax.numpy as jnp  # JAX NumPy

from clu import metrics
from flax.training import train_state  # Useful dataclass to keep train state
from flax import struct  # Flax dataclasses
import optax  # Common loss functions and optimizers


@struct.dataclass
class Metrics(metrics.Collection):
    accuracy: metrics.Accuracy
    loss: metrics.Average.from_output("loss")


class TrainState(train_state.TrainState):
    metrics: Metrics


def create_train_state(module, rng, learning_rate, momentum):
    """Creates an initial `TrainState`."""
    params = module.init(rng, jnp.ones([1, 28, 28, 1]))[
        "params"
    ]  # initialize parameters by passing a template image
    tx = optax.sgd(learning_rate, momentum)
    return TrainState.create(
        apply_fn=module.apply, params=params, tx=tx, metrics=Metrics.empty()
    )


@jax.jit
def train_step(state, batch):
    """Train for a single step."""

    def loss_fn(params):
        logits = state.apply_fn({"params": params}, batch["image"])
        loss = optax.softmax_cross_entropy_with_integer_labels(
            logits=logits, labels=batch["label"]
        ).mean()
        return loss

    grad_fn = jax.grad(loss_fn)
    grads = grad_fn(state.params)
    state = state.apply_gradients(grads=grads)
    return state


@jax.jit
def compute_metrics(*, state, batch):
    logits = state.apply_fn({"params": state.params}, batch["image"])
    loss = optax.softmax_cross_entropy_with_integer_labels(
        logits=logits, labels=batch["label"]
    ).mean()
    metric_updates = state.metrics.single_from_model_output(
        logits=logits, labels=batch["label"], loss=loss
    )
    metrics = state.metrics.merge(metric_updates)
    state = state.replace(metrics=metrics)
    return state


class CNN(nn.Module):
    """A simple CNN model."""

    @nn.compact
    def __call__(self, x):
        x = nn.Conv(features=32, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = nn.Conv(features=64, kernel_size=(3, 3))(x)
        x = nn.relu(x)
        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
        x = x.reshape((x.shape[0], -1))  # flatten
        x = nn.Dense(features=256)(x)
        x = nn.relu(x)
        x = nn.Dense(features=10)(x)
        return x


def get_datasets(num_epochs, batch_size):
    train_ds = tfds.load("mnist", split="train")
    test_ds = tfds.load("mnist", split="test")

    train_ds = train_ds.map(
        lambda sample: {
            "image": tf.cast(sample["image"], tf.float32) / 255.0,
            "label": sample["label"],
        }
    )  # normalize train set
    test_ds = test_ds.map(
        lambda sample: {
            "image": tf.cast(sample["image"], tf.float32) / 255.0,
            "label": sample["label"],
        }
    )  # normalize test set

    train_ds = train_ds.repeat(num_epochs).shuffle(
        1024
    )  # create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from
    train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(
        1
    )  # group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency
    test_ds = test_ds.shuffle(
        1024
    )  # create shuffled dataset by allocating a buffer size of 1024 to randomly draw elements from
    test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(
        1
    )  # group into batches of batch_size and skip incomplete batch, prefetch the next sample to improve latency

    return train_ds, test_ds


cnn = CNN()
print(
    cnn.tabulate(
        jax.random.key(0),
        jnp.ones((1, 28, 28, 1)),
        compute_flops=True,
        compute_vjp_flops=True,
    )
)
